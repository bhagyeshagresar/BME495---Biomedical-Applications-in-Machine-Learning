{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Project 1 (100 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this project, you need to build a Logistic Regression model from scratch to classify patients with and without a heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At first, import the necessary libraries. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After that, upload the data and preprocess it. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data, separate the features and the labels into X and y variables\n",
    "data = pd.read_csv('heart_disease_classification.csv')\n",
    "y = data['target']\n",
    "X = data.drop(['target'], axis = 1)\n",
    "\n",
    "# One-hot-encode the necessary columnns (which columns do we need to one-hot-encode?)\n",
    "#columns cp, thal and scope need to be one hot encoded\n",
    "a = pd.get_dummies(X['cp'], prefix = \"cp\")\n",
    "b = pd.get_dummies(X['thal'], prefix = \"thal\")\n",
    "c = pd.get_dummies(X['slope'], prefix = \"slope\")\n",
    "\n",
    "X_expanded = pd.concat([X, a, b, c], axis=1)\n",
    "\n",
    "X = X_expanded.drop(columns = ['cp', 'thal', 'slope']).astype('float')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the features\n",
    "X = (X - np.min(X)) / (np.max(X) - np.min(X)).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before starting to code the Logistic Regression model, check if your data is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  sex  trestbps      chol  fbs  restecg   thalach  exang   oldpeak  \\\n",
      "0  0.708333  1.0  0.481132  0.244292  1.0      0.0  0.603053    0.0  0.370968   \n",
      "1  0.166667  1.0  0.339623  0.283105  0.0      0.5  0.885496    0.0  0.564516   \n",
      "2  0.250000  0.0  0.339623  0.178082  0.0      0.0  0.770992    0.0  0.225806   \n",
      "3  0.562500  1.0  0.245283  0.251142  0.0      0.5  0.816794    0.0  0.129032   \n",
      "4  0.583333  0.0  0.245283  0.520548  0.0      0.5  0.702290    1.0  0.096774   \n",
      "\n",
      "    ca  ...  cp_1  cp_2  cp_3  thal_0  thal_1  thal_2  thal_3  slope_0  \\\n",
      "0  0.0  ...   0.0   0.0   1.0     0.0     1.0     0.0     0.0      1.0   \n",
      "1  0.0  ...   0.0   1.0   0.0     0.0     0.0     1.0     0.0      1.0   \n",
      "2  0.0  ...   1.0   0.0   0.0     0.0     0.0     1.0     0.0      0.0   \n",
      "3  0.0  ...   1.0   0.0   0.0     0.0     0.0     1.0     0.0      0.0   \n",
      "4  0.0  ...   0.0   0.0   0.0     0.0     0.0     1.0     0.0      0.0   \n",
      "\n",
      "   slope_1  slope_2  \n",
      "0      0.0      0.0  \n",
      "1      0.0      0.0  \n",
      "2      0.0      1.0  \n",
      "3      0.0      1.0  \n",
      "4      0.0      1.0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets. Use a 80-20 split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you can start building your Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You need a bunch of helper functions. Let's start with initialize and sigmoid. (10 points each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the initialize function (Hint: Look up how to use the np.random sublibrary)\n",
    "# It needs one input: dimension\n",
    "# It will return two outputs: random weights and random bias\n",
    "def initialize(dimension):\n",
    "    weights = np.random.rand(dimension,1)\n",
    "    weights = weights.flatten()\n",
    "    bias = np.random.rand()\n",
    "#     bias = bias.flatten()\n",
    "    \n",
    "    return  weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69646919 0.28613933 0.22685145 0.55131477 0.71946897 0.42310646]\n",
      "0.9807641983846155\n"
     ]
    }
   ],
   "source": [
    "# Check if your function works with the following test case\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "weights,bias = initialize(6)\n",
    "\n",
    "print(weights)\n",
    "print(bias)\n",
    "\n",
    "#Should be weights = [0.69646919 0.28613933 0.22685145 0.55131477 0.71946897 0.42310646] and bias = 0.9807641983846155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sigmoid function (one input, one output - formula given in the slides)\n",
    "def sigmoid(X):\n",
    "    \n",
    "    sigma = 1/(1+np.exp(-X))\n",
    "    \n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.0066928509242848554\n",
      "0.9525741268224334\n"
     ]
    }
   ],
   "source": [
    "# Check if your function works with the following test cases\n",
    "\n",
    "print(sigmoid(0)) #should be 0.5\n",
    "\n",
    "print(sigmoid(-5)) #should be 0.00669\n",
    "\n",
    "print(sigmoid(3)) #should be 0.95257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next two helper functions will be calculate_cost_gradient and GradientDescent. (20 points each) The functions we have written for Linear Regression can help you but keep in mind that the cost function is crossentropy and the gradients will change accordingly. \n",
    "\n",
    "#### Also, you need to include the tol value to GradientDescent, so it stops at convergence. Make sure the GradientDescent function returns the cost after every iteration, prints the final cost and plots the cost vs number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the calculate_cost_gradient function\n",
    "def calculate_cost_gradient(weights, bias, X_train, y_train):\n",
    "    #gradient descent step 1\n",
    "    y_output = np.dot(X_train, weights) + bias\n",
    "    cost = (y_train*np.log(sigmoid(y_output)) + (1-y_train)*np.log(1-sigmoid(y_output)))\n",
    "    cost = -np.sum(cost)/X_train.shape[0]\n",
    "    \n",
    "    derivative_weight = -np.dot(X_train.T, y_train - sigmoid(y_output))/X_train.shape[0]\n",
    "    derivative_bias = -np.sum(y_train - sigmoid(y_output))/X_train.shape[0]\n",
    "    \n",
    "    gradient = {'weight_gradient': derivative_weight, 'bias_gradient': derivative_bias}\n",
    "    \n",
    "    return cost, gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the GradientDescent function (Do not forget to include tol as an extra input)\n",
    "def GradientDescent(old_weights, bias, x_train, y_train, lr, iters, tol):\n",
    "    \n",
    "    costList = []\n",
    "    index = []\n",
    "    \n",
    "    for ii in range(iters):\n",
    "        \n",
    "        cost, gradients = calculate_cost_gradient(old_weights, bias, X_train, y_train)\n",
    "        \n",
    "        new_weights = old_weights - lr*gradients['weight_gradient']\n",
    "        bias = bias - lr*gradients['bias_gradient']\n",
    "        \n",
    "        costList.append(cost)\n",
    "        index.append(ii)\n",
    "    \n",
    "        if(np.linalg.norm(new_weights - old_weights) < tol):\n",
    "            break\n",
    "        old_weights = new_weights \n",
    "        \n",
    "    model_parameters = {'weights': new_weights, 'bias': bias}\n",
    "    \n",
    "    print('iterations:', iters)\n",
    "    print('cost:', cost)\n",
    "    \n",
    "    #plot to see how the cost decreases\n",
    "    \n",
    "    plt.plot(index, costList)\n",
    "    plt.xlabel('num of iters')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.show()\n",
    "    \n",
    "    return model_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last helper function you need to write is predict. (10 points) It takes the model parameters and the test data as inputs and returns the test predictions. (Predicted labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the predict function\n",
    "def predict(model_parameters, X_test):\n",
    "    \n",
    "    w = model_parameters[\"weights\"]\n",
    "    b = model_parameters[\"bias\"]\n",
    "    \n",
    "    y_temp =  np.dot(X_test, w) + b\n",
    "    \n",
    "    y_pred = [0 for x in range(len(y_temp))]\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    for i in range(len(y_temp)):\n",
    "        if(sigmoid(y_temp[i]) >= 0.5):\n",
    "            y_pred[i] = 1\n",
    "        else:\n",
    "            y_pred[i] = 0\n",
    "        \n",
    "       \n",
    "  \n",
    "    \n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Check if your function works with the following test case\n",
    "\n",
    "x_testcase = np.array([[1, 3, 4 ,7],[4, 6, -7, -9], [1, 3, 5, 7]])\n",
    "params = {'weights':[1, 2, 3, 4], 'bias':5}\n",
    "\n",
    "print(predict(params,x_testcase)) #Should be [1, 0 ,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lastly, you need to write the logistic regression function. (5 points) Keep in mind that you need to initialize the parameters, optimize them, get the predictions for test data and evaluate the performance of the model. The evaluation metric for classification is accuracy. Make sure the function prints the test accuracy like the linear regression function we wrote in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the logistic regression function.\n",
    "def logistic_regression(X_train, y_train, X_test, y_test, lr, iters, tol):\n",
    "    \n",
    "    #Initialize\n",
    "    init_weights, init_bias = initialize(X_train.shape[1])\n",
    "    \n",
    "    print(\"init_weights\", init_weights.shape)\n",
    "    print(\"init_bias\", init_bias)\n",
    "    #Train the ML model\n",
    "    parameters = GradientDescent(init_weights, init_bias, X_train, y_train, lr, iters, tol)\n",
    "    \n",
    "    \n",
    "#     print(X_test.shape)\n",
    "    print(\"weights\", parameters[\"weights\"])\n",
    "    print(\"bias\", parameters[\"bias\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Get predictions for test data\n",
    "    y_pred = predict(parameters, X_test)\n",
    "    \n",
    "#     print(\"y_pred\", y_pred)\n",
    "    \n",
    "    #Evaluate\n",
    "    print(\"Test accuracy: {:.2f}\".format((100*np.mean(y_pred == y_test))) + '%')\n",
    "    \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, you need to run your logistic regression function with the training and the test datasets defined at the beginning.\n",
    "\n",
    "#### The stepsize (learning rate), number of iterations and tolerance values will be your choice. Tune them until the model matches the accuracy we found in class with the built-in model. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_weights (21,)\n",
      "init_bias 0.29371404638882936\n",
      "iterations: 1000\n",
      "cost: 0.4200200719278874\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAg50lEQVR4nO3deXhdd33n8fdXV1f7ZknX8iLbso1jY6dOQkSWltC0KeAEHigFSkKAQGEymUIL7bQsndI+dHlaBtqBliWTCSG0ZGAyNGVNk3RYEgpxEhkSJ46dxHa8yLGtzdqXq3v1nT/OlawokixbPjpXOp/X89zn3LPce74+j6WPzvn9zu+YuyMiIvFVEHUBIiISLQWBiEjMKQhERGJOQSAiEnMKAhGRmCuMuoCzVV9f701NTVGXISKyqOzatavD3VPTrVt0QdDU1ERLS0vUZYiILCpmdnimdbo0JCIScwoCEZGYUxCIiMRcaEFgZneYWZuZPTXLNleb2eNmtsfMHgyrFhERmVmYZwR3AjtmWmlmNcAXgTe6+zbgbSHWIiIiMwgtCNz9IaBrlk3eAdzj7kdy27eFVYuIiMwsyjaCC4BlZvZjM9tlZu+eaUMzu9nMWsyspb29fQFLFBFZ+qIMgkLgUuD1wOuAT5jZBdNt6O63uXuzuzenUtPeD3FG+0708qn79tEzNHrOBYuILEVRBkErcJ+7D7h7B/AQcFFYOzvaNcSXfnyAQx0DYe1CRGRRijIIvg1cZWaFZlYGXA7sDWtna2pLATh6ajCsXYiILEqhDTFhZl8HrgbqzawV+HMgCeDut7r7XjO7D9gNjAG3u/uMXU3na82yMgCOdCkIREQmCy0I3P2GOWzzaeDTYdUwWXlxIXXlRRxVEIiIvEis7ixeU1vG0a6hqMsQEckrsQqCtbVlujQkIjJFrIJgTW0px7qHyGTHoi5FRCRvxCoI1taWkR1zjvcMR12KiEjeiFUQrKkNeg6pwVhE5LR4BYG6kIqIvESsgmBldQmFBaabykREJolVEBQmClhVU8oRdSEVEZkQqyCAoMFYbQQiIqfFLgjWKAhERF4khkFQSudAmoGRTNSliIjkhdgFwdrxLqRqMBYRAWIYBBNdSDsVBCIiEMMgGD8j0L0EIiKB2AVBTVmSyuJCWk+pC6mICMQwCMyMRo1CKiIyIXZBALC2tlRBICKSE8sgWLMsuJfA3aMuRUQkcrEMgrV1ZYxkxmjvG4m6FBGRyMUyCNao55CIyIR4BsEy3VQmIjIulkHQuKwUgCOd6kIqIhLLIChJJlhRVaJLQyIixDQIIBh8TqOQiojEOAjW1ZVzqHMg6jJERCIXWhCY2R1m1mZmT51hu1eaWdbM3hpWLdNpqiujrW+EwbSGoxaReAvzjOBOYMdsG5hZAvgUcH+IdUyrqb4cgMMahVREYi60IHD3h4CuM2z2e8C/AG1h1TGTprrxINDlIRGJt8jaCMxsNfBm4NY5bHuzmbWYWUt7e/t52f/auuBegkM6IxCRmIuysfizwEfdPXumDd39NndvdvfmVCp1XnZeVZKkrryIQx06IxCReCuMcN/NwDfMDKAeuM7MMu7+rYUqYF1dmXoOiUjsRRYE7r5+/L2Z3Ql8byFDAIIG44cPdC7kLkVE8k6Y3Ue/DjwMbDazVjN7n5ndYma3hLXPs9VUV87xnmGGR894dUpEZMkK7YzA3W84i23fE1Yds1lXd3oU0gsaKqMoQUQkcrG9sxhOdyFVg7GIxJmCANRgLCKxFusgqC5LUlOW1L0EIhJrsQ4CCM4KdHexiMSZgqCujEMdOiMQkfiKfRCsqyvnhZ4hRjLqQioi8RT7IGiqL8MdPaRGRGIr9kGwbqILqYJAROIp9kGwXl1IRSTmYh8ENWVJqkoK9YAaEYmt2AeBmdFUr+cXi0h8xT4IIGgn0BmBiMSVgoDgXoLWU4OkM2NRlyIisuAUBAR3F485tJ7SWYGIxI+CgOBeAlDPIRGJJwUBsL6+AoCD7QoCEYkfBQFQW17EsrIkBxQEIhJDCoKcjakKDrT3R12GiMiCUxDkbExVcFBBICIxpCDI2bi8nI7+NN2D6ahLERFZUAqCnI2poMFY7QQiEjcKgpzxINDlIRGJGwVBTuOyUooSBTojEJHYURDkFCYKaKovU88hEYkdBcEk6kIqInEUWhCY2R1m1mZmT82w/kYz2517/czMLgqrlrnakCrnSOcgo1kNPici8RHmGcGdwI5Z1j8P/Kq7bwf+ErgtxFrmZGOqgsyYa0hqEYmV0ILA3R8CumZZ/zN3P5Wb3Qk0hlXLXJ3uQqrLQyISH/nSRvA+4N9mWmlmN5tZi5m1tLe3h1bEhlTw/GIFgYjESeRBYGa/RhAEH51pG3e/zd2b3b05lUqFVktlSZKGqmIOtKkLqYjER2GUOzez7cDtwLXu3hllLeM2pio42KEzAhGJj8jOCMxsLXAP8C53fzaqOqbamKrgQFs/7h51KSIiCyK0MwIz+zpwNVBvZq3AnwNJAHe/FfgzoA74opkBZNy9Oax65mpjqpze4Qwd/WlSlcVRlyMiErrQgsDdbzjD+vcD7w9r/+dq4/LTPYcUBCISB5E3FucbdSEVkbhREEyxoqqEsqKEeg6JSGwoCKYoKDA2pMp1RiAisaEgmMbGVAX72xQEIhIPCoJpbFpewbHuIfpHMlGXIiISOgXBNLasqALgmRN9EVciIhI+BcE0Nq+oBBQEIhIPCoJpNC4rpaK4kGdO9EZdiohI6BQE0zAzNq+oZK/OCEQkBhQEM9i8opJnTvRpzCERWfIUBDPYsqKSnqFRTvaORF2KiEioFAQz2NwQNBjvUzuBiCxxCoIZjHch3ad2AhFZ4hQEM6guS7KyukRdSEVkyVMQzGLzikqdEYjIkqcgmMWWFVXsb+tjNDsWdSkiIqFREMxiy4pKRrPO8x0aklpEli4FwSzGh5rQ5SERWcoUBLPYmKqgsMDYd1xdSEVk6ZpTEJjZP89l2VJTVFjAxlSFeg6JyJI21zOCbZNnzCwBXHr+y8k/6jkkIkvdrEFgZh83sz5gu5n15l59QBvw7QWpMGKbV1RyrHuI3uHRqEsREQnFrEHg7n/j7pXAp929KveqdPc6d//4AtUYqS25BuNndVYgIkvUXC8Nfc/MygHM7J1m9vdmti7EuvLGlpXBUBMaklpElqq5BsGXgEEzuwj4CHAY+KfQqsojq6pLqClL8vQLPVGXIiISirkGQcaDgfnfBHzO3T8HVM72ATO7w8zazOypGdabmf2Dme03s91m9oqzK31hmBm/tLqax48qCERkaZprEPSZ2ceBdwHfz/UaSp7hM3cCO2ZZfy2wKfe6meCsIy9d1FjDsyf7GEpnoy5FROS8m2sQvB0YAX7H3U8Aq4FPz/YBd38I6JplkzcB/+SBnUCNma2cYz0LantjNdkx5+njOisQkaVnTkGQ++V/F1BtZm8Aht19vm0Eq4Gjk+Zbc8tewsxuNrMWM2tpb2+f527P3kVragB4QpeHRGQJmuudxb8NPAq8Dfht4BEze+s8923TLJv2AcHufpu7N7t7cyqVmuduz15DVQkNVcXsbu1e8H2LiIStcI7b/Tfgle7eBmBmKeD/Ad+cx75bgTWT5huBF+bxfaHa3ljDE606IxCRpWeubQQF4yGQ03kWn53Jd4B353oPXQH0uPvxeX5naC5eU8PzHQP0DOkOYxFZWuZ6RnCfmd0PfD03/3bg3tk+YGZfB64G6s2sFfhzcj2N3P3W3OevA/YDg8B7z7b4hbS9sRqAJ1t7eNWm+oirERE5f2YNAjN7GdDg7n9sZr8FvIrg2v7DBI3HM3L3G86w3oEPnF250dm+ugaAJ1q7FQQisqSc6fLOZ4E+AHe/x93/0N3/gOCv+c+GW1p+qS5L0lRXpgZjEVlyzhQETe6+e+pCd28BmkKpKI9tb6xhtxqMRWSJOVMQlMyyrvR8FrIYbG+s5njPMG29w1GXIiJy3pwpCB4zs/80daGZvQ/YFU5J+WvixjKdFYjIEnKmXkMfBv7VzG7k9C/+ZqAIeHOIdeWlbauqSBQYu1u7ec3WhqjLERE5L2YNAnc/Cfyymf0acGFu8ffd/YehV5aHyooK2bS8QmcEIrKkzOk+Anf/EfCjkGtZFC5qrOH+p0/g7phNN0qGiMjiMt+7g2Pn4rU1dA+OcqB9IOpSRETOCwXBWbp8fS0AjzzfGXElIiLnh4LgLK2vL6ehqpiHDygIRGRpUBCcJTPjig117DzYRTBKhojI4qYgOAdXbqijo3+EA+39UZciIjJvCoJzcMWGOgAePjjbkzhFRBYHBcE5WFdXxsrqEnaqnUBElgAFwTk43U7QqXYCEVn0FATn6MoNdXQOpHmuTe0EIrK4KQjO0Xg7wc6DujwkIoubguAcraktZXVNqe4nEJFFT0FwjsyMyzfU8sjzXYyNqZ1ARBYvBcE8XLmhjq6BNM+29UVdiojIOVMQzMNEO4EuD4nIIqYgmIc1tWWsqS3lP/Z3RF2KiMg5UxDM0zVbGvjJcx0MpjNRlyIick4UBPP02m0NjGTGePCZ9qhLERE5J6EGgZntMLNnzGy/mX1smvXVZvZdM3vCzPaY2XvDrCcMlzXVUlOW5IGnT0ZdiojIOQktCMwsAXwBuBbYCtxgZlunbPYB4Gl3vwi4Gvg7MysKq6YwFCYKuGZLAz/Ye5LR7FjU5YiInLUwzwguA/a7+0F3TwPfAN40ZRsHKi14+G8F0AUsuovtr9vWQO9wRncZi8iiFGYQrAaOTppvzS2b7PPAy4EXgCeBD7n7ovuz+tUXpChNJrh/z4moSxEROWthBoFNs2zqLbivAx4HVgEXA583s6qXfJHZzWbWYmYt7e351yhbkkzwqxek+PenT+ouYxFZdMIMglZgzaT5RoK//Cd7L3CPB/YDzwNbpn6Ru9/m7s3u3pxKpUIreD5eu62Bk70jPNHaHXUpIiJnJcwgeAzYZGbrcw3A1wPfmbLNEeAaADNrADYDB0OsKTTXbGmgsMC4f496D4nI4hJaELh7BvggcD+wF7jb3feY2S1mdktus78EftnMngR+AHzU3RflbbrVZUmu2FDHA3tO6GE1IrKoFIb55e5+L3DvlGW3Tnr/AvDaMGtYSK/b1sAnvr2HZ0/2s3lFZdTliIjMie4sPo92XLiSZMK4u+XomTcWEckTCoLzKFVZzGu3reCbu1oZHs1GXY6IyJwoCM6zGy9bS8/QKPc+eTzqUkRE5kRBcJ5dubGO9fXl3PXIkahLERGZEwXBeWZmvOOytew6fIpnTujJZSKS/xQEIXjLpY0UJQr4348cjroUEZEzUhCEoLa8iGt/aQX3/PyYHlgjInlPQRCSGy9fR99Ihu89oUZjEclvCoKQvLJpGZuWV/DVhw/pTmMRyWsKgpCYGTe/egN7Xujlvqc0PLWI5C8FQYjefMlqNqbK+cwDz5DV8NQikqcUBCEqTBTwR6/dzIH2Af71F8eiLkdEZFoKgpDtuHAFv7S6mv/x788yktGwEyKSfxQEITMz/vh1mznWPcQ3HtVgdCKSfxQEC+CqTfVcvr6Wf/zhft1XICJ5R0GwAMyMj+zYTEf/CP/wg/1RlyMi8iIKggVy6bpa3t68htseOsCuw11RlyMiMkFBsID+9A0vZ2V1KX949xO6RCQieUNBsIAqS5J85m0XcbhzkL+5d1/U5YiIAAqCBXflxjp+51fW8887D/OT59qjLkdEREEQhY/s2MzGVDl/9H+f4ETPcNTliEjMKQgiUJJM8I83vIL+4Qzv+cqj9A6PRl2SiMSYgiAiW1dVceu7LmV/Wz//5Wu7SGfGoi5JRGJKQRChqzal+NRbtvPT/Z185JtPMKaB6UQkAoVRFxB3b7m0kRO9w3z6/meoKCnkk2+8kESBRV2WiMRIqGcEZrbDzJ4xs/1m9rEZtrnazB43sz1m9mCY9eSr3716I//51Rv42s4j/O5duxge1eB0IrJwQgsCM0sAXwCuBbYCN5jZ1inb1ABfBN7o7tuAt4VVTz4zMz5+3cv5xBu28sDTJ7nx9kc4NZCOuiwRiYkwzwguA/a7+0F3TwPfAN40ZZt3APe4+xEAd28LsZ68975XrefzN7yCJ4/18JZbf8a+E71RlyQiMRBmEKwGJo+73JpbNtkFwDIz+7GZ7TKzd0/3RWZ2s5m1mFlLe/vSvgnr9dtX8rX3XU7vUIY3fv6nfOWnz+uZxyISqjCDYLoWz6m/0QqBS4HXA68DPmFmF7zkQ+63uXuzuzenUqnzX2meuWx9Lfd9+Cquelk9n/zu09z0lcdo69WNZyISjjCDoBVYM2m+EXhhmm3uc/cBd+8AHgIuCrGmRaO+opjbb2rmL3/zQh452Mk1f/cgt//kIKNZ3W8gIudXmEHwGLDJzNabWRFwPfCdKdt8G7jKzArNrAy4HNgbYk2LipnxrivW8W8fuopXrFvGX31/Lzs++xAPPru0L4+JyMIKLQjcPQN8ELif4Jf73e6+x8xuMbNbctvsBe4DdgOPAre7+1Nh1bRYbUhVcOd7X8mXb2omM+bcdMejXH/bw/xsf4faD0Rk3myx/SJpbm72lpaWqMuIzEgmy107j3Drgwdo6xvh0nXL+OCvvYxfvSBFgW5EE5EZmNkud2+edp2CYHEaHs1yd8tRvvTjAxzvGaaprox3XdnEWy9tpLo0GXV5IpJnFARLWDozxr89dZyv/uwQPz/STWkyweu3r+Qtr2jk8vW1OksQEUBBEBtPtvbwtZ2H+f6Tx+kfybC6ppQ3X7KaN1y0ks0NlZgpFETiSkEQM0PpLPfvOcG//LyVn+7vYMxhQ6qc6y5cyY4LV7BtVZVCQSRmFAQx1t43wv17TnDvk8fZebCTMYeGqmJ+fUsD12xZzpUb6ygv1iC0IkudgkAA6Owf4Yf72vjhvjYeeradgXSWZMK4dN0yrtqU4lUvq2fbqioKE3pMhchSoyCQlxjJZHns+VP8ZH87Dz3bwd7jwQB3lcWFXLa+lis21HHZ+lq2rqoiqWAQWfQUBHJGbX3D7DzYxcMHOnnkYCcHOwYAKE0muGRtDc3rlnHJumVcsqaGmrKiiKsVkbOlIJCzdrJ3mMcOddFy6BSPHepi7/Fexp+kuaG+nIvW1LC9sZrtjdVsW1VNSTIRbcEiMisFgczbwEiG3a09/OLoKX5xpJvdrd2c7B0BIFFgbEyVc+GqarauqgpeK6t05iCSR2YLAnUXkTkpLy7kyo11XLmxbmLZyd5hdrf2sLu1mz0v9PLTAx3c84tjE+tXVpewZUUlm1dUsXlFBRc0VLIxVaGzB5E8oyCQc9ZQVcJrtpbwmq0NE8va+obZd7yPvcd72Xu8l30n+viP/R2MZoMzzwKDdXXlbExVsKmhgo2pCjamytmQqtDQGCIRURDIebW8soTllSW8+oLTDxAazY5xqGOAfSf6eK6tnwNt/TzX1seDz7ZNBAQEz2DYUF9OU30Z6+sraKorY11dOevqynSvg0iI9NMloUsmCtjUUMmmhsoXLR/NjnG0a5CD7QMcaO/nQHs/hzoG+dEz7dzd0vqibesrillXV8ba2jLW1JaxZlkpa2rLaFxWysrqUhIaU0nknCkIJDLJRAEbUhVsSFXwGzS8aF3f8CiHOwc53DnIoc4BDncOcKRrkEef7+Jbjx9jch+HwgJjRXUJq2tKWb2slMaaUlbWlLKyuoRVuWlliS47icxEQSB5qbIkyYWrq7lwdfVL1qUzY7zQPUTrqSGOnhqk9dQgx04Ncax7iJ0HOjnROzzR1XXi+4oLWVFdEryqgmlDVfB+eVUxDVUl1JUX6a5qiSUFgSw6RYUFNNWX01RfPu36THaMk30jHO8OwuFEzzDHe4Y53hO8f/ZkH+19Iy8JiwKD2vJillcWs7wqmKYqi0lVFFNfWUx9xfiriOrSpAbukyVDQSBLTmGiILhMVFPKtJ2mCcKioz/Nyd5hTvYO09Y3Qtv4tG+Etr5h9h7vpbM/TWZqYgDJhFFbXkRdeTF1FUXUlhfl5ouoLS+emK8tT7KsLAgOnW1IvlIQSCwVJgomLhXNZmzM6R4apb1vhI7+4BW8T9M1MELXQJr2/jSHOgfo6k8zkM7O+F3VpUmWlSWpKSuamNaUJakpzU3LklSVJqkuTVKTm1aVJjXWk4ROQSAyi4ICm/jrfjOVZ9x+eDRL10CaU4NpTg2M0jkwQvfgKF0DaboH03QNjtI9mKajP81zbf10D47SP5KZ9TvLihJUjwdDSZKq0kIqS5JUlRRSVZqksiSYrywppKpkfP70stJkQpexZFYKApHzqCSZYFVNKatqSuf8mdHsGL1Do3QPjdIzNErPYG465dU3HExf6B6mZ6iPvuEgRKa5cvUiBQYVxUEwVBQXUl6coKIkSUVxIjdfSEXuVZ5bX140/r6Q8qJEblpIWXFCZyhLkIJAJGLJRAF1FcXUVRSf9Wfdnf6RDH3D46/RYDoSvO/PLe8fyeS2G2VgJJsLlCH6hzMMjGToT2eY67BjRYkCynJhUVqUoLwokZsG82VFCcrG3yeDdePLS5PB8tJkMF8yvj4ZvEqSBTp7iYCCQGQRM7PcJaD53Sfh7gymswyMZBjITftHMgymMwyMnJ4fSmcn1g+mswyNBtPBkSwn+4Yn3g+mMwyNZl905/hclSQLJgXD+KsgCI3cfHFuvqTw9LqSZAHFk+aLCwsozm1TnCyYmBYXTlpfmCCZsNiHj4JARDCziUtB59NodiwIjHSWodEgIIZHswylxybCIpjPMjQ6xlA6w3BmbGLZcGaMoXSWkUww3zs8GiwfHWMkE0yHR7PT9uyaK7PgLGc8OIKACEKiKPe+KDdf/KL5YFpUWEBRInH6fWEBxYnJ6wpetG58PjkxNYpzn08mLJLeZQoCEQlNMlFAdWlB6AMKjmbHGMmMMTKanQiSkdExhjNB0ATrxsMjSzoTbD/5ffDKnn6f+1w6M0bfcIaOTJp05vSy8Wk6O0Z2HkE0VYEFx60oUUAyFw7jwfGOy9by/qs2nLd9jQs1CMxsB/A5IAHc7u5/O8N2rwR2Am9392+GWZOILD3JRPCLsiKiwQmzYx6EQmaMkWwQQqPZICTSmbGJoEpPCo/R7BijGWdk0jYT09y6dDbLaMYnlqUqz74daS5CO2pmlgC+ALwGaAUeM7PvuPvT02z3KeD+sGoREQlTosAmGsVh8Y1rFebFqMuA/e5+0N3TwDeAN02z3e8B/wK0hViLiIjMIMwgWA0cnTTfmls2wcxWA28Gbp3ti8zsZjNrMbOW9vb2816oiEichRkE0/XHmtqi8lngo+4+8335gLvf5u7N7t6cSqVm21RERM5SmC0rrcCaSfONwAtTtmkGvpHrw1sPXGdmGXf/Voh1iYjIJGEGwWPAJjNbDxwDrgfeMXkDd18//t7M7gS+pxAQEVlYoQWBu2fM7IMEvYESwB3uvsfMbsmtn7VdQEREFkaonW7d/V7g3inLpg0Ad39PmLWIiMj0NIygiEjMmc91yME8YWbtwOFz/Hg90HEey1lqdHxmp+MzMx2b2eXD8Vnn7tN2u1x0QTAfZtbi7jM9vTD2dHxmp+MzMx2b2eX78dGlIRGRmFMQiIjEXNyC4LaoC8hzOj6z0/GZmY7N7PL6+MSqjUBERF4qbmcEIiIyhYJARCTmYhMEZrbDzJ4xs/1m9rGo64mSma0xsx+Z2V4z22NmH8otrzWzfzez53LTZVHXGiUzS5jZL8zse7l5HZ8cM6sxs2+a2b7c/6MrdXwCZvYHuZ+rp8zs62ZWku/HJhZBMOlpadcCW4EbzGxrtFVFKgP8V3d/OXAF8IHc8fgY8AN33wT8IDcfZx8C9k6a1/E57XPAfe6+BbiI4DjF/vjknrHy+0Czu19IMM7a9eT5sYlFEDD3p6XFgrsfd/ef5973EfwQryY4Jl/NbfZV4DcjKTAPmFkj8Hrg9kmLdXwAM6sCXg18GcDd0+7ejY7PuEKg1MwKgTKC4ffz+tjEJQjO+LS0uDKzJuAS4BGgwd2PQxAWwPIIS4vaZ4GPAGOTlun4BDYA7cBXcpfObjezcnR8cPdjwGeAI8BxoMfdHyDPj01cgmAuT0uLHTOrIHhe9IfdvTfqevKFmb0BaHP3XVHXkqcKgVcAX3L3S4AB8uxSR1Ry1/7fBKwHVgHlZvbOaKs6s7gEwVyelhYrZpYkCIG73P2e3OKTZrYyt34l0BZVfRH7FeCNZnaI4DLir5vZ19DxGdcKtLr7I7n5bxIEg44P/AbwvLu3u/socA/wy+T5sYlLEEw8Lc3Miggab74TcU2RseDZoF8G9rr7309a9R3gptz7m4BvL3Rt+cDdP+7uje7eRPB/5Yfu/k50fABw9xPAUTPbnFt0DfA0Oj4QXBK6wszKcj9n1xC0weX1sYnNncVmdh3Bdd/xp6X9dbQVRcfMXgX8BHiS09fA/4SgneBuYC3Bf+i3uXtXJEXmCTO7Gvgjd3+DmdWh4wOAmV1M0JBeBBwE3kvwh2Xsj4+ZfRJ4O0HvvF8A7wcqyONjE5sgEBGR6cXl0pCIiMxAQSAiEnMKAhGRmFMQiIjEnIJARCTmFAQi55mZpczskdzwC1dNWXf7+ICHZvYn0VQo8mLqPipynpnZ9cC17n7TGbbrd/eKs/zuhLtn51WgyBQ6I5Alx8yacmPk/6/cuPAPmFlpbt2Pzaw5974+N4wEZvYeM/uWmX3XzJ43sw+a2R/m/qrfaWa10+xnnZn9wMx256Zrczda/XfgOjN7fHy/kz7zYzNrNrO/JRih8nEzuyu37p1m9mhu2f/MDZ+OmfWb2V+Y2SPAlWb2t2b2dG6/nwnvSEpcKAhkqdoEfMHdtwHdwFvm8JkLgXcQDFv+18BgblC1h4F3T7P954F/cvftwF3AP7j748CfAf/H3S9296HpduTuHwOGctvcaGYvJ7gb9Vfc/WIgC9yY27wceMrdLycYyuHNwLbcfv9qDv8ukVkVRl2ASEiez/1SBtgFNM3hMz/KPZ+hz8x6gO/mlj8JbJ9m+yuB38q9/2eCM4FzdQ1wKfBYMEQNpZwemCxLMEAgQC8wDNxuZt8HvjePfYoACgJZukYmvc8S/GKFYPyX8TPhklk+MzZpfoy5/azMp8HNgK+6+8enWTc83i7g7hkzu4wgOK4HPgj8+jz2K6JLQxI7hwj+8gZ46zy/62cEv4whuIzzH2f5+dHccOAQPL7wrWa2HCaej7xu6gdyz5Codvd7gQ8DF59D3SIvojMCiZvPAHeb2buAH87zu34fuMPM/pjgiV3vPcvP3wbsNrOf59oJ/hR4wMwKgFHgA8DhKZ+pBL5tZiUEZxF/MK9/gQjqPioiEnu6NCQiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjGnIBARiTkFgYhIzP1/+xS92v6gEeEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights [ 0.14036837 -0.56109736  0.01673382  0.13323739  0.29767462  0.34299887\n",
      "  0.0478082  -0.47406742  0.23110673 -0.56970689 -0.70379655  0.65176924\n",
      "  0.74282024  0.61851864  0.82807229  0.46747044  1.23817292 -0.19028175\n",
      "  0.16235102 -0.37753217  0.56427184]\n",
      "bias -0.27020606194129154\n",
      "Test accuracy: 86.89%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'weights': array([ 0.14036837, -0.56109736,  0.01673382,  0.13323739,  0.29767462,\n",
       "         0.34299887,  0.0478082 , -0.47406742,  0.23110673, -0.56970689,\n",
       "        -0.70379655,  0.65176924,  0.74282024,  0.61851864,  0.82807229,\n",
       "         0.46747044,  1.23817292, -0.19028175,  0.16235102, -0.37753217,\n",
       "         0.56427184]),\n",
       " 'bias': -0.27020606194129154}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression(X_train, y_train, X_test, y_test, lr=0.2, iters=1000, tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Accuracy is 86.89%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
